{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Project Rule:\n",
        "**This project measures reasoning ability, not language fluency.**"
      ],
      "metadata": {
        "id": "_cr5VV--JGUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8djyd23JKUW",
        "outputId": "2ec6df00-fd51-4beb-8435-0b83da549e94"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chosen Task: SCAN – compositional command understanding**\n",
        "**Evaluation: Accuracy on unseen command compositions**"
      ],
      "metadata": {
        "id": "-pbCeQTBKDoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Success Criteria\n",
        "# Model size ≤ 10M parameters\n",
        "# Beats a 50–100M baseline on SCAN generalization split\n",
        "# No chain-of-thought output\n",
        "# Accuracy is the only metric"
      ],
      "metadata": {
        "id": "dEY5YElHKMXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrCAUz4DJhOw",
        "outputId": "f2365f7a-7c5b-4286-862b-a81286e86e19"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n"
      ],
      "metadata": {
        "id": "Rs4ilc_1XGkj"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folders = [\n",
        "    \"data\",\n",
        "    \"models\",\n",
        "    \"training\",\n",
        "    \"evaluation\",\n",
        "    \"experiments\"\n",
        "]\n",
        "\n",
        "for f in folders:\n",
        "    os.makedirs(f, exist_ok=True)\n",
        "\n",
        "print(\"Project structure ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1RDyxWtKaGZ",
        "outputId": "9712b769-9987-4b5c-cc86-3e54d434f18b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project structure ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SCAN is:\n",
        "\n",
        "Input: command string\n",
        "\n",
        "\"jump around right twice\"\n",
        "\n",
        "\n",
        "Output: action sequence\n",
        "\n",
        "JUMP RTURN JUMP RTURN\n",
        "\n",
        "Decisions (write this in a Markdown cell):\n",
        "\n",
        "Token-level sequence-to-sequence\n",
        "\n",
        "Teacher forcing\n",
        "\n",
        "Cross-entropy loss\n",
        "\n",
        "Exact-match accuracy"
      ],
      "metadata": {
        "id": "i8RQLXXvOkRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: Vocabulary setup\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(sequences):\n",
        "    counter = Counter()\n",
        "    for seq in sequences:\n",
        "        counter.update(seq.split())\n",
        "    vocab = {tok: i+2 for i, tok in enumerate(counter.keys())}\n",
        "    vocab[\"<pad>\"] = 0\n",
        "    vocab[\"<eos>\"] = 1\n",
        "    return vocab\n"
      ],
      "metadata": {
        "id": "_GxCCE-dKgZW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(seq, vocab):\n",
        "    return [vocab[token] for token in seq.split()] + [vocab[\"<eos>\"]]\n"
      ],
      "metadata": {
        "id": "ED4-0HOYOrLk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ScanDataset(Dataset):\n",
        "    def __init__(self, inputs, outputs, in_vocab, out_vocab):\n",
        "        self.inputs = inputs\n",
        "        self.outputs = outputs\n",
        "        self.in_vocab = in_vocab\n",
        "        self.out_vocab = out_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(encode(self.inputs[idx], self.in_vocab))\n",
        "        y = torch.tensor(encode(self.outputs[idx], self.out_vocab))\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "SofWXATEOw_b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ReasoningBottleneck(nn.Module):\n",
        "    def __init__(self, d_model, n_slots=4, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.n_slots = n_slots\n",
        "\n",
        "        self.slots = nn.Parameter(torch.randn(1, n_slots, d_model))\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim=d_model,\n",
        "            num_heads=n_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # ✅ Stabilizer\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, D)\n",
        "        B = x.size(0)\n",
        "\n",
        "        # Expand slots across batch\n",
        "        slots = self.slots.expand(B, -1, -1)  # (B, n_slots, D)\n",
        "\n",
        "        # Slots attend to tokens\n",
        "        slots, _ = self.attn(\n",
        "            query=slots,\n",
        "            key=x,\n",
        "            value=x\n",
        "        )\n",
        "\n",
        "        # ✅ Stabilize representations\n",
        "        slots = self.norm(slots)\n",
        "\n",
        "        return slots  # (B, n_slots, D)\n"
      ],
      "metadata": {
        "id": "vgxROZoARxNu"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_in, vocab_out, d_model=128, n_heads=4, n_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_in = nn.Embedding(vocab_in, d_model)\n",
        "        self.embed_out = nn.Embedding(vocab_out, d_model)\n",
        "\n",
        "        self.pos_enc = nn.Parameter(torch.zeros(1, 512, d_model))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, n_layers)\n",
        "        self.reasoning = ReasoningBottleneck(d_model=d_model, n_slots=6)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, n_layers)\n",
        "\n",
        "        self.output = nn.Linear(d_model, vocab_out)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.embed_in(src) + self.pos_enc[:, :src.size(1)]\n",
        "        tgt = self.embed_out(tgt) + self.pos_enc[:, :tgt.size(1)]\n",
        "\n",
        "        encoded = self.encoder(src)\n",
        "        memory = self.reasoning(encoded)\n",
        "\n",
        "        out = self.decoder(tgt, memory)\n",
        "        return self.output(out)\n"
      ],
      "metadata": {
        "id": "AklCcGNXO16m"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model, batch, optimizer, criterion):\n",
        "    model.train()\n",
        "    src, tgt = batch\n",
        "    tgt_input = tgt[:, :-1]\n",
        "    tgt_output = tgt[:, 1:]\n",
        "\n",
        "    logits = model(src, tgt_input)\n",
        "    loss = criterion(\n",
        "        logits.reshape(-1, logits.size(-1)),\n",
        "        tgt_output.reshape(-1)\n",
        "    )\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n"
      ],
      "metadata": {
        "id": "0YhRng7yO9X9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/brendenlake/SCAN.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn_spCCqPD7f",
        "outputId": "70e15965-e22d-4a45-e80d-1ac2bc906493"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SCAN'...\n",
            "remote: Enumerating objects: 205, done.\u001b[K\n",
            "remote: Total 205 (delta 0), reused 0 (delta 0), pack-reused 205 (from 1)\u001b[K\n",
            "Receiving objects: 100% (205/205), 11.10 MiB | 14.88 MiB/s, done.\n",
            "Resolving deltas: 100% (173/173), done.\n",
            "Updating files: 100% (212/212), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_scan(path):\n",
        "    inputs, outputs = [], []\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                inp, out = line.split(\" OUT: \")\n",
        "                inp = inp.replace(\"IN: \", \"\")\n",
        "                inputs.append(inp)\n",
        "                outputs.append(out)\n",
        "    return inputs, outputs\n",
        "\n",
        "\n",
        "train_inputs, train_outputs = load_scan(\n",
        "    \"SCAN/simple_split/tasks_train_simple.txt\"\n",
        ")\n",
        "\n",
        "test_inputs, test_outputs = load_scan(\n",
        "    \"SCAN/length_split/tasks_test_length.txt\"\n",
        ")\n",
        "\n",
        "print(len(train_inputs), len(test_inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdX_hGVHPass",
        "outputId": "b1360700-bfb5-42b7-b2ba-cda569b2005c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16728 3920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_vocab = build_vocab(train_inputs)\n",
        "out_vocab = build_vocab(train_outputs)\n",
        "\n",
        "print(len(in_vocab), len(out_vocab))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCrz6w_zQBvO",
        "outputId": "342096c0-fa1b-458f-d29c-2d70938620be"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    xs = pad_sequence(xs, batch_first=True, padding_value=0)\n",
        "    ys = pad_sequence(ys, batch_first=True, padding_value=0)\n",
        "    return xs.cuda(), ys.cuda()\n"
      ],
      "metadata": {
        "id": "Xo5BI1RxQUps"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = ScanDataset(\n",
        "    train_inputs, train_outputs, in_vocab, out_vocab\n",
        ")\n",
        "\n",
        "test_dataset = ScanDataset(\n",
        "    test_inputs, test_outputs, in_vocab, out_vocab\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n"
      ],
      "metadata": {
        "id": "h9RtfAaWQYdA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "\n",
        "model = TinyTransformer(\n",
        "    vocab_in=len(in_vocab),\n",
        "    vocab_out=len(out_vocab)\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "Xwp6T_RJQaSD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n"
      ],
      "metadata": {
        "id": "Oekkh5SMQcbd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        loss = train_step(model, batch, optimizer, criterion)\n",
        "        total_loss += loss\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0glKa97QesI",
        "outputId": "dbc52356-f06d-49c7-e72d-9ab880deac8d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 0.0016\n",
            "Epoch 2 | Loss: 0.0017\n",
            "Epoch 3 | Loss: 0.0008\n",
            "Epoch 4 | Loss: 0.0014\n",
            "Epoch 5 | Loss: 0.0009\n",
            "Epoch 6 | Loss: 0.0004\n",
            "Epoch 7 | Loss: 0.0024\n",
            "Epoch 8 | Loss: 0.0019\n",
            "Epoch 9 | Loss: 0.0007\n",
            "Epoch 10 | Loss: 0.0016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in loader:\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            logits = model(src, tgt_input)\n",
        "            preds = logits.argmax(-1)\n",
        "\n",
        "            for i in range(preds.size(0)):\n",
        "                if torch.equal(preds[i], tgt[i, 1:preds.size(1)+1]):\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "acc = evaluate(model, test_loader)\n",
        "print(\"Test Accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFg6iPg-Qkim",
        "outputId": "a3900313-8699-4278-e50e-e1b2d7676f67"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.4813775510204082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = evaluate(model, test_loader)\n",
        "print(\"Test Accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEogvbYAQx-C",
        "outputId": "becb6760-7139-45f4-aebd-c01a809278ee"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.42423469387755103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seeds = [1, 2, 3, 4, 5]\n",
        "results = []\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f\"\\nRunning seed {seed}\")\n",
        "    set_seed(seed)\n",
        "\n",
        "    model = TinyTransformer(\n",
        "        vocab_in=len(in_vocab),\n",
        "        vocab_out=len(out_vocab)\n",
        "    ).cuda()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        for batch in train_loader:\n",
        "            train_step(model, batch, optimizer, criterion)\n",
        "\n",
        "    acc = evaluate(model, test_loader)\n",
        "    results.append(acc)\n",
        "\n",
        "    print(f\"Seed {seed} → Test Acc = {acc:.4f}\")\n",
        "\n",
        "print(\"\\nFinal Results:\")\n",
        "print(\"Mean:\", sum(results) / len(results))\n",
        "print(\"Std:\", torch.tensor(results).std().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B48h5pvgTPE1",
        "outputId": "aa14d81c-25b5-4eaa-b12c-5b5b3ec3d356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running seed 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EbnATwZwXSfT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}