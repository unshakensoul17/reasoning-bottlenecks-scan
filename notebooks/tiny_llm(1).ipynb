{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Project Rule:\n",
        "**This project measures reasoning ability, not language fluency.**"
      ],
      "metadata": {
        "id": "_cr5VV--JGUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8djyd23JKUW",
        "outputId": "2ec6df00-fd51-4beb-8435-0b83da549e94"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chosen Task: SCAN â€“ compositional command understanding**\n",
        "**Evaluation: Accuracy on unseen command compositions**"
      ],
      "metadata": {
        "id": "-pbCeQTBKDoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Success Criteria\n",
        "# Model size â‰¤ 10M parameters\n",
        "# Beats a 50â€“100M baseline on SCAN generalization split\n",
        "# No chain-of-thought output\n",
        "# Accuracy is the only metric"
      ],
      "metadata": {
        "id": "dEY5YElHKMXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrCAUz4DJhOw",
        "outputId": "f2365f7a-7c5b-4286-862b-a81286e86e19"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n"
      ],
      "metadata": {
        "id": "Rs4ilc_1XGkj"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folders = [\n",
        "    \"data\",\n",
        "    \"models\",\n",
        "    \"training\",\n",
        "    \"evaluation\",\n",
        "    \"experiments\"\n",
        "]\n",
        "\n",
        "for f in folders:\n",
        "    os.makedirs(f, exist_ok=True)\n",
        "\n",
        "print(\"Project structure ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1RDyxWtKaGZ",
        "outputId": "9712b769-9987-4b5c-cc86-3e54d434f18b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project structure ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SCAN is:\n",
        "\n",
        "Input: command string\n",
        "\n",
        "\"jump around right twice\"\n",
        "\n",
        "\n",
        "Output: action sequence\n",
        "\n",
        "JUMP RTURN JUMP RTURN\n",
        "\n",
        "Decisions (write this in a Markdown cell):\n",
        "\n",
        "Token-level sequence-to-sequence\n",
        "\n",
        "Teacher forcing\n",
        "\n",
        "Cross-entropy loss\n",
        "\n",
        "Exact-match accuracy"
      ],
      "metadata": {
        "id": "i8RQLXXvOkRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: Vocabulary setup\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(sequences):\n",
        "    counter = Counter()\n",
        "    for seq in sequences:\n",
        "        counter.update(seq.split())\n",
        "    vocab = {tok: i+2 for i, tok in enumerate(counter.keys())}\n",
        "    vocab[\"<pad>\"] = 0\n",
        "    vocab[\"<eos>\"] = 1\n",
        "    return vocab\n"
      ],
      "metadata": {
        "id": "_GxCCE-dKgZW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(seq, vocab):\n",
        "    return [vocab[token] for token in seq.split()] + [vocab[\"<eos>\"]]\n"
      ],
      "metadata": {
        "id": "ED4-0HOYOrLk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ScanDataset(Dataset):\n",
        "    def __init__(self, inputs, outputs, in_vocab, out_vocab):\n",
        "        self.inputs = inputs\n",
        "        self.outputs = outputs\n",
        "        self.in_vocab = in_vocab\n",
        "        self.out_vocab = out_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(encode(self.inputs[idx], self.in_vocab))\n",
        "        y = torch.tensor(encode(self.outputs[idx], self.out_vocab))\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "SofWXATEOw_b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class QuantumBottleneck(nn.Module):\n",
        "    def __init__(self, d_model, n_slots=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_slots = n_slots\n",
        "        self.compress = nn.Linear(d_model, n_qubits)\n",
        "        self.expand = nn.Linear(n_qubits, d_model)\n",
        "\n",
        "        self.q_weights = nn.Parameter(torch.randn(n_qubits))\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # x: (B, T, D)\n",
        "      B = x.size(0)\n",
        "\n",
        "      pooled = x.mean(dim=1)              # (B, D)\n",
        "      compressed = self.compress(pooled)  # (B, n_qubits)\n",
        "\n",
        "      # ğŸ”¹ One quantum call per batch\n",
        "      batch_vec = compressed.mean(dim=0)  # (n_qubits,)\n",
        "      q_vec = quantum_feature_map(batch_vec, self.q_weights)\n",
        "      q_vec = torch.stack(q_vec)           # (n_qubits,)\n",
        "\n",
        "      q_out = q_vec.unsqueeze(0).repeat(B, 1)  # (B, n_qubits)\n",
        "\n",
        "      expanded = self.expand(q_out)\n",
        "      expanded = self.norm(expanded)\n",
        "\n",
        "      slots = expanded.unsqueeze(1).repeat(1, self.n_slots, 1)\n",
        "      return slots\n"
      ],
      "metadata": {
        "id": "vgxROZoARxNu"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_in, vocab_out, d_model=128, n_heads=4, n_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_in = nn.Embedding(vocab_in, d_model)\n",
        "        self.embed_out = nn.Embedding(vocab_out, d_model)\n",
        "\n",
        "        self.pos_enc = nn.Parameter(torch.zeros(1, 512, d_model))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, n_layers)\n",
        "        self.reasoning = QuantumBottleneck(d_model=d_model, n_slots=4)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, n_layers)\n",
        "\n",
        "        self.output = nn.Linear(d_model, vocab_out)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.embed_in(src) + self.pos_enc[:, :src.size(1)]\n",
        "        tgt = self.embed_out(tgt) + self.pos_enc[:, :tgt.size(1)]\n",
        "\n",
        "        encoded = self.encoder(src)\n",
        "        memory = self.reasoning(encoded)\n",
        "\n",
        "        out = self.decoder(tgt, memory)\n",
        "        return self.output(out)\n"
      ],
      "metadata": {
        "id": "AklCcGNXO16m"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model, batch, optimizer, criterion):\n",
        "    model.train()\n",
        "    src, tgt = batch\n",
        "    tgt_input = tgt[:, :-1]\n",
        "    tgt_output = tgt[:, 1:]\n",
        "\n",
        "    logits = model(src, tgt_input)\n",
        "    loss = criterion(\n",
        "        logits.reshape(-1, logits.size(-1)),\n",
        "        tgt_output.reshape(-1)\n",
        "    )\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n"
      ],
      "metadata": {
        "id": "0YhRng7yO9X9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/brendenlake/SCAN.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn_spCCqPD7f",
        "outputId": "70e15965-e22d-4a45-e80d-1ac2bc906493"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SCAN'...\n",
            "remote: Enumerating objects: 205, done.\u001b[K\n",
            "remote: Total 205 (delta 0), reused 0 (delta 0), pack-reused 205 (from 1)\u001b[K\n",
            "Receiving objects: 100% (205/205), 11.10 MiB | 14.88 MiB/s, done.\n",
            "Resolving deltas: 100% (173/173), done.\n",
            "Updating files: 100% (212/212), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_scan(path):\n",
        "    inputs, outputs = [], []\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                inp, out = line.split(\" OUT: \")\n",
        "                inp = inp.replace(\"IN: \", \"\")\n",
        "                inputs.append(inp)\n",
        "                outputs.append(out)\n",
        "    return inputs, outputs\n",
        "\n",
        "\n",
        "train_inputs, train_outputs = load_scan(\n",
        "    \"SCAN/simple_split/tasks_train_simple.txt\"\n",
        ")\n",
        "\n",
        "test_inputs, test_outputs = load_scan(\n",
        "    \"SCAN/length_split/tasks_test_length.txt\"\n",
        ")\n",
        "\n",
        "print(len(train_inputs), len(test_inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdX_hGVHPass",
        "outputId": "b1360700-bfb5-42b7-b2ba-cda569b2005c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16728 3920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_vocab = build_vocab(train_inputs)\n",
        "out_vocab = build_vocab(train_outputs)\n",
        "\n",
        "print(len(in_vocab), len(out_vocab))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCrz6w_zQBvO",
        "outputId": "342096c0-fa1b-458f-d29c-2d70938620be"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    xs = pad_sequence(xs, batch_first=True, padding_value=0)\n",
        "    ys = pad_sequence(ys, batch_first=True, padding_value=0)\n",
        "    return xs.cuda(), ys.cuda()\n"
      ],
      "metadata": {
        "id": "Xo5BI1RxQUps"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = ScanDataset(\n",
        "    train_inputs, train_outputs, in_vocab, out_vocab\n",
        ")\n",
        "\n",
        "test_dataset = ScanDataset(\n",
        "    test_inputs, test_outputs, in_vocab, out_vocab\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n"
      ],
      "metadata": {
        "id": "h9RtfAaWQYdA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "\n",
        "model = TinyTransformer(\n",
        "    vocab_in=len(in_vocab),\n",
        "    vocab_out=len(out_vocab)\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "Xwp6T_RJQaSD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n"
      ],
      "metadata": {
        "id": "Oekkh5SMQcbd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        loss = train_step(model, batch, optimizer, criterion)\n",
        "        total_loss += loss\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0glKa97QesI",
        "outputId": "dbc52356-f06d-49c7-e72d-9ab880deac8d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 0.0016\n",
            "Epoch 2 | Loss: 0.0017\n",
            "Epoch 3 | Loss: 0.0008\n",
            "Epoch 4 | Loss: 0.0014\n",
            "Epoch 5 | Loss: 0.0009\n",
            "Epoch 6 | Loss: 0.0004\n",
            "Epoch 7 | Loss: 0.0024\n",
            "Epoch 8 | Loss: 0.0019\n",
            "Epoch 9 | Loss: 0.0007\n",
            "Epoch 10 | Loss: 0.0016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in loader:\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            logits = model(src, tgt_input)\n",
        "            preds = logits.argmax(-1)\n",
        "\n",
        "            for i in range(preds.size(0)):\n",
        "                if torch.equal(preds[i], tgt[i, 1:preds.size(1)+1]):\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "acc = evaluate(model, test_loader)\n",
        "print(\"Test Accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFg6iPg-Qkim",
        "outputId": "a3900313-8699-4278-e50e-e1b2d7676f67"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.4813775510204082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seeds = [1, 2, 3, 4, 5]\n",
        "results = []\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f\"\\nRunning seed {seed}\")\n",
        "    set_seed(seed)\n",
        "\n",
        "    model = TinyTransformer(\n",
        "        vocab_in=len(in_vocab),\n",
        "        vocab_out=len(out_vocab)\n",
        "    ).cuda()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        for batch in train_loader:\n",
        "            train_step(model, batch, optimizer, criterion)\n",
        "\n",
        "    acc = evaluate(model, test_loader)\n",
        "    results.append(acc)\n",
        "\n",
        "    print(f\"Seed {seed} â†’ Test Acc = {acc:.4f}\")\n",
        "\n",
        "print(\"\\nFinal Results:\")\n",
        "print(\"Mean:\", sum(results) / len(results))\n",
        "print(\"Std:\", torch.tensor(results).std().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B48h5pvgTPE1",
        "outputId": "62cfb418-c00a-43db-ec2f-f69119b7a12b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running seed 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantum interference"
      ],
      "metadata": {
        "id": "8jUUeq_lfGY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pennylane pennylane-lightning\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbnATwZwXSfT",
        "outputId": "f21b00e0-0c1a-40db-ebce-f6fe0fad8c29"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading pennylane-0.44.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pennylane-lightning\n",
            "  Downloading pennylane_lightning-0.44.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.16.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.6.1)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray==0.8.2 (from pennylane)\n",
            "  Downloading autoray-0.8.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (7.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.4)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (26.0)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.0.2)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning)\n",
            "  Downloading scipy_openblas32-0.3.31.22.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.7.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2026.1.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.46.3)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n",
            "Downloading pennylane-0.44.0-py3-none-any.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.8.2-py3-none-any.whl (935 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m935.6/935.6 kB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pennylane_lightning-0.44.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy_openblas32-0.3.31.22.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, scipy-openblas32, rustworkx, autoray, diastatic-malt, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.8.2 diastatic-malt-2.15.2 pennylane-0.44.0 pennylane-lightning-0.44.0 rustworkx-0.17.1 scipy-openblas32-0.3.31.22.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "n_qubits = 4\n",
        "dev = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\", diff_method=\"parameter-shift\")\n",
        "def quantum_feature_map(inputs, weights):\n",
        "    # Angle encoding\n",
        "    for i in range(n_qubits):\n",
        "        qml.RY(inputs[i], wires=i)\n",
        "\n",
        "    # Entanglement\n",
        "    for i in range(n_qubits - 1):\n",
        "        qml.CNOT(wires=[i, i + 1])\n",
        "\n",
        "    # Trainable layer\n",
        "    for i in range(n_qubits):\n",
        "        qml.RY(weights[i], wires=i)\n",
        "\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n"
      ],
      "metadata": {
        "id": "DFeTkSh5fEKk"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Xa2n-mDfS3x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}